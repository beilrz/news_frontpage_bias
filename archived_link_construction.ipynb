{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e59680",
   "metadata": {},
   "source": [
    "## Check to collect sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def clean_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    netloc_without_www = parsed_url.netloc.lstrip('www.')\n",
    "    cleaned_url = netloc_without_www + parsed_url.path\n",
    "    \n",
    "    if cleaned_url.endswith('/'):\n",
    "        cleaned_url = cleaned_url[:-1]\n",
    "    if cleaned_url.startswith(\"www.\"):\n",
    "        cleaned_url = cleaned_url[4:]\n",
    "    return cleaned_url\n",
    "\n",
    "df_sw = pd.read_csv(\"data/similarWeb_ranking_2023-6.csv\")\n",
    "\n",
    "allsides_urls = [clean_url(url) for url in df[\"link\"]]\n",
    "similarwebs_urls = [clean_url(url) for url in df_sw[\"Website\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_additional = df_sw[df_sw[\"Website\"].isin(list(set(similarwebs_urls) - set(allsides_urls)))]\n",
    "df_additional = df_additional.drop(columns = [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4298b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_additional[\"info\"] = \"N/A\"\n",
    "df_additional.to_csv(\"additonal_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1568c2",
   "metadata": {},
   "source": [
    "## Contstuct links to be collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent function from access global namespace\n",
    "import types\n",
    "noglobal = lambda f: types.FunctionType(f.__code__, {}, argdefs=f.__defaults__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def construct_json_url(url, matchType, limit, _filter, _from, _to, _collapse):\n",
    "    link = f\"http://web.archive.org/cdx/search/cdx?url={url}&matchType={matchType}&limit={limit}\"\\\n",
    "        + f\"&filter={_filter}&from={_from}&to={_to}&collapse={_collapse}\"\n",
    "\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.dailymail.co.uk\"\n",
    "matchType = \"exact\"\n",
    "limit = \"10\"\n",
    "_filter = \"statuscode:301|302\"\n",
    "_from = 2022\n",
    "_to= 2023\n",
    "_collapse = \"timestamp:10\"\n",
    "    \n",
    "print(construct_json_url(url, matchType, limit, _filter, _from, _to, _collapse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://web.archive.org/cdx/search/cdx?url=https://www.dailymail.co.uk&matchType=exact&limit=13500&from=2022&to=2023&collapse=timestamp:10\"\n",
    "file_name = \"Daily-Mail_13500_202201_202306.txt\"\n",
    "\n",
    "r = get_link_with_proxy(url)\n",
    "with open(\"data/site_links/\" + file_name, \"w\") as outfile:\n",
    "    outfile.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c8fbc",
   "metadata": {},
   "source": [
    "## Collect links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4acc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchType = \"exact\"\n",
    "limit = \"13500\" # 13500\n",
    "_filter = \"statuscode:200\"\n",
    "_from = 202201\n",
    "_to= 202306\n",
    "_collapse = \"timestamp:10\"\n",
    "    \n",
    "for x in df.iterrows():\n",
    "    x = x[1]\n",
    "    url = x[\"link\"]\n",
    "    name = x[\"news\"]\n",
    "    \n",
    "    name = name.replace(\" \", \"-\")\n",
    "    file_name = f\"{name}_{limit}_{_from}_{_to}.txt\"\n",
    "    print(file_name)\n",
    "    \n",
    "    file_url = construct_json_url(url, matchType, limit, _filter, _from, _to, _collapse)\n",
    "    r = get_link_with_proxy(file_url)\n",
    "    \n",
    "    with open(\"data/site_links/\" + file_name, \"w\") as outfile:\n",
    "        outfile.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c14e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/site_links/test.txt\", \"w\") as outfile:\n",
    "    outfile.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c01e4",
   "metadata": {},
   "source": [
    "### select datepoint to collect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
